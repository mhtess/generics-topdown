---
title             : "Statistical knowledge mediates the influence of dangerousness on generic language"
shorttitle        : "Rethinking generics and dangerousness"

author: 
  - name          : "Michael Henry Tessler"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "450 Serra Mall, Bldg. 420, Rm. 316, Stanford, CA 94305"
    email         : "mhtessler@stanford.edu"
  - name          : "Noah D. Goodman"
    affiliation   : "1"

affiliation:
  - id            : "1"
    institution   : "Department of Psychology, Stanford University"
    
header-includes:
  - \usepackage{tabularx}
  - \usepackage{multicol}
  - \usepackage{wrapfig}
  - \usepackage{caption}
  - \usepackage{booktabs}
  - \usepackage{amsmath}
  - \usepackage{graphicx}
  - \usepackage{subcaption}
  - \usepackage{longtable}
  - \usepackage{array}
  - \usepackage{multirow}
  - \usepackage{xcolor}
  
author_note: >
  This manuscript is currently in prep. Comments or suggestions should be directed to MH Tessler.

abstract: |
  Enter abstract here. Each new line herein must be indented, like this line.
  
keywords          : "keywords"
wordcount         : "X"

bibliography      : ["generics.bib"]

figsintext        : yes
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : no
mask              : no

class             : "man"
output            : papaja::apa6_pdf
---
\newcommand{\denote}[1]{\mbox{ $[\![ #1 ]\!]$}}
\newcommand*\diff{\mathop{}\!\mathrm{d}}
\definecolor{Red}{RGB}{255,0,0}
\definecolor{Green}{RGB}{10,200,100}
\definecolor{Blue}{RGB}{10,100,200}

\newcommand{\mht}[1]{{\textcolor{Blue}{[mht: #1]}}}
\newcommand{\ndg}[1]{{\textcolor{Green}{[ndg: #1]}}}
\newcommand{\red}[1]{{\textcolor{Red}{#1}}}


```{r load_packages, include = FALSE}
library(papaja)
library(tidyverse)
library(cowplot)
library(ggthemes)
library(RColorBrewer)
library(ggpirate)
library(brms)
theme_set(theme_few())
```

```{r helper functions}

format_regression_effects <- function(brm_summary, fixed_effect_name, n_digits = 3){
   #print(fixed_effect_name)
   e1 <- brm_summary[["fixed"]][[fixed_effect_name, "Estimate"]]
   e_lower <- brm_summary[["fixed"]][[fixed_effect_name, "l-95% CI"]]
   e_upper <- brm_summary[["fixed"]][[fixed_effect_name, "u-95% CI"]]
   return(paste(
     format(e1, digits = n_digits), " [", 
     format(e_lower, digits = n_digits), ", ", 
     format(e_upper, digits = n_digits), "]", sep = ""))
}

compute_r2 <- function(df,v1, v2, sigfigs = 3){
  return(format(cor(df[[v1]], df[[v2]])^2, digits = sigfigs))
}

compute_mse <- function(df, v1, v2, sigfigs = 3){
  return(format(mean( (df[[v1]]-df[[v2]])^2), digits = sigfigs))
}


```


```{r analysis_preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(echo=FALSE, warning=FALSE, cache=T, message=FALSE, sanitize = T)
```


# Introduction

Generic language (*generics*, e.g., "Alligators have big teeth") convey generalizations about categories [@Carlson1977].
Generics are believed to be a part of every language [@behrens], and they are seemingly understood by children as early as they have the syntactic knowledge to combine a subject and a predicate [@graham:30mo].
The fact that generics refer to abstractions outside of direct experience (i.e., categories) has pinpointed them as potentially central in learning abstract knowledge about the world [@Gelmanbook].
The question of what generic statements mean has thus of interest to psychologists, linguists, and philosophers for at least forty years.

There is considerable experimental evidence that meaning of generic statements cannot simply be reduced to probabilistic or statistical statements [@Carlson1995; @Gelman2007; @Leslie2008; @Prasada2000; @Cimpian2010theory; @Cimpian2010]: Generics are judged true even when the predicated property is only rarely present (e.g., "Mosquitos carry malaria" is true despite very few mosquitos actually carrying malaria).
Instead, a popular view in cognitive science is sthat generics are a direct, linguistic manifestion of conceptual knowledge[@Prasada; @Leslie].  
As a result, the process of understanding generics is predicted to be sensitive to certain top-down influence.
Accordingly, @Leslie2007 brings our attention to examples like the following:

1. Sharks attack bathers.
2. Rottweilers maul children. 
3. Mosquitos carry the West Nile Virus.

\noindent arguing that these generics are reasonable utterances, even though the actual *prevalence* of the property (e.g., the number of mosquitos that carry the West Nile Virus) is quite low because in each case, the property is somehow *striking, alarming,* or *dangerous*:

> The examples... have something in common: In all of them, the sentence attributes harmful, dangerous, or appalling properties to the kind. More generally, if the property in question is the sort of property of which one would be well served to be forewarned, even if there were only a small chance of encountering it, then generic attributions of the property are intuitively true. (2008, p.15)

\noindent Consistent with this viewpoint, @Cimpian2010 found participants willing to endorse generics about striking properties (e.g., "Lorches have dangerous feathers") more so than that of neutral properties (e.g., "Lorches have purple feathers") at low levels of prevalence (e.g., when only 30% of the category have the property).

<!-- @Cimpian2010 found that generics about novel categories (e.g., "Lorches have purple feathers") are endorsed more highly when the property being predicated is *distinctive and dangerous*  (Expt. 1) and when described independently as either distinctive or dangerous (Expt. 4). -->
<!-- For example, the utterance "Rottweilers maul children" is thought to be a true generic statement  -->
<!-- not because most rottweilers maul children, but  -->
<!-- because *mauling children* is a striking property for animal to exhibit and the human conceptual system priviledges this kind of survival-relevant information [@Leslie2008]. -->


Recently, however, we have shown that generics *can* be reduced to probabilistic statements, albeit ones that are vague and context-dependent: A generic is true if an instance of the category is subjectively likely to have the property, where what counts as *likely* is determined by one's prior knowledge about the property in a set of alternative categories [@TesslerGenericsInPress].
For instance, *carrying malaria* is significantly more true of mosquitos than it is of other insects or animals.
This framework is sufficiently general to explain the context-sensitivity of *habitual language* (e.g., "John runs") as well, a relationship that has been remarked upon in the literature on generics [@Carlson; @Leslie] but never before foramlized. 

Here, we examine and test an intruiging claim of the computational model of @TesslerGenericsInPress: Whether differences in endorsements based on top-down, conceptual knowledge (e.g., strikingness) could be mediated by corresponding differences in the relevant prior knowledge recruited about the properties in the form of expectations about prevalence.^[
  Another non-mutually exclusive possibility suggested by @Leslie2008 is that a speaker's perception of the prevalence of the feature is altered by virtue of its dangerousness [@Rothbart1978].
]
If this were so, we would expect information about the strikingness of a property to alter the listener's relevant prior knowledge to reflect the kind of prior knowledge associated with *distinctive* properties (i.e., properties that are expected to be present in very few categories).
We examine this claim in the context of the experimental paradigm of @Cimpian2010, which found judgments about generics were influenced by information about the dangerousness of the property being predicated. 
We first replicate the findings of @Cimpian2010 Expt. 4, which showed that information about the dangerousness of a property increases generic endorsement at low prevalence levels.
Then, we run a prevalence elicitation experiments to see if these differences in endorsements are associated with differences in the prevalence prior distributions.
We use the computational model of @TesslerGenericsInPress to predict the endorsement results using the elicited priors, finding that indeed the priors are sufficiently altered as the result of information about the property's dangerousness to account for the differences in endorsements observed by @Cimpian2010.
Finally, we perform a Bayesian mediation analysis, finding that probabilistic knowledge fully mediates the effects of dangerous information on generic endorsements.
These results call into question claims about the special status of dangerous information on generic language understanding and suggest more careful experimental controls for future investigations.

# Computational model predictions

@TesslerGenericsInPress propose a computational model of truth judgments or endorsements about generic sentences.
This model is a kind of Rational Speech Act model [@goodman] wherein a rational speaker decides whether or not to produce an utterance to a Bayesian listener. 
The listener $L$ updates his prior beliefs from the meaning of the utterance, and the speaker $S$ decides if the generic (or an information-less alternative) would better align the listener's beliefs with that of their own.
The model is given by these two equations:

\begin{eqnarray}
S(u \mid r) \propto (\int_{\theta} L(r, \theta \mid u)  \diff \theta)^\lambda  \label{eq:S1} \\
L(r, \theta \mid u) &\propto& {\delta_{\denote{u}(r, \theta)} \cdot P(r) \cdot P(\theta)} \label{eq:L0} 
\end{eqnarray}

Using the truth-functional tools of formal semantics [@Montague1973], the literal meaning of a generic statement is modeled as a threshold function operating on the prevalence $r$ of the feature in the category (e.g., the proportion of mosquitos that carry malaria): $\denote{generic} = \{r > \theta\}$.^[
  The prevalence $r$ can be thought as a "prevalence in the mind", or a subjective degree of belief about the property in the category. Such subjective probabilities may or may not accurately track true empirical frequencies.
]
The literal meaning of quantifiers can also be described by threshold functions (e.g., $\denote{some} = \{r > 0\}$, $\denote{most} = \{r > 0.5\}$, $\denote{all} = \{r = 1\}$).
@TesslerLangGenUnderReview posit that the corresponding threshold for the generic $\theta$ is *a priori* uncertain---formally, it is drawn from a context-invariant, uniform prior distribution on thresholds $P(\theta)$---and is resolved by context.


# Experiment 1

@Cimpian2010 Expt. 4 demonstrated that generic statements ascribing dangerous properties to novel categories as well as those ascribing distinctive properties to novel categories were judged more true at low-levels of prevalence (e.g., when only 30% of the category had the property) than control items.
Here, we aim to directly replicate this finding using the same stimuli. 

## Method

### Participants

We recruited 80 participants over MTurk. 
The experimental design is very similar to @Cimpian2010 (Expt. 4), and we chose to have a sample size at least twice as large as the original study (original n=15). 
All participants were native English speakers. 
The experiment took about 4 minutes and participants were compensated \$0.50.

### Materials and procedure

In order to get participants motivated to reason about novel kinds, they were told they were the resident zoologist of a team of scientists on a recently discovered island with many unknown animals; their task was to provide their expert opinion on questions about these animals.
On each trial, participants were given a statement about a property's prevalence within a novel kind (*referent prevalence*; e.g. "50% of feps have yellow fur."). 
Participants were then asked whether or not they agreed or disagreed with the corresponding generic sentence (e.g. "Feps have yellow fur.").

The materials were taken from @Cimpian2010 (Expt. 4).
The properties were 10 animal body parts (e.g., feathers, scales, shells, tails, ...), and each body part was paired with three different colors, creating a total of 30 properties. 
In addition, sentences conveying the dangerousness and the distinctiveness of the property were created and paired with each item.
Finally, property prevalence was either 10%, 30%, 50%, 70%, and 90%.
For example, a dangerous property statement appeared as:

> 30% of lorches have dangerous purple feathers. These feathers are as sharp as needles and can easily get lodged in you, causing massive bleeding.

Participants were then asked if they agreed or disagreed with the generic: "Lorches have dangerous purple feathers". 
Distinctive information was supplemented with some nondangerous facts.
For example, a distinctive property information appeared as: 

> 30% of lorches have distinctive purple feathers. No other animals on this island have wide, smooth feathers like these.

Participants then evaluated a generic such as "Lorches have distinctive purple feathers". 
The experiment consisted of thirty trials. Participant saw each prevalence level twice paired with each of the three kinds of property (5 prevalence levels x 2 repetitions x 3 property types).

The experiment can be viewed at http://stanford.edu/~mtessler/generics/cbg2010-replication/experiment/experiment-15.html

## Results

```{r load dangerous endorsement results, cache=F}
load("cached_results/dangerous-endorsements-summary.RData") # d.summary
load("cached_results/dangerous-endorsements-randIntTypeeff.RData") # brms.endorsement.bern.1, brms.endorsement.bern.2,
brms.endorsement.bern.2.summary <- summary(brms.endorsement.bern.2)
```

One participant was excluded for self-reporting a native language other than English, leaving n=79 for these analyses.
Figure\ \@ref(fig:dangerousEndorsements) shows the endorsement levels for the three different kinds of properties at the five prevalence levels tested.
We see that both dangerous and distinctive properties have a different pattern of endorsement than the control properties.
This is confirmed by a Bayesian Bernoulli regression model with fixed-effects of property type (dangerous vs. distinctive vs. neither), prevalence levels (treated as a continuous variable, centered and scaled), and their interaction.
As well, the model included random intercepts and effects of property type, by participant and by item. 
We find evidence for different slopes such that both dangerous properties ($\beta_{dangerous\times prevalence}=$ `r format_regression_effects(brms.endorsement.bern.2.summary, "stim_typedanger:prev_centered")`) and distinctive properties ($\beta_{distinctive\times prevalence}=$ `r format_regression_effects(brms.endorsement.bern.2.summary, "stim_typedistinct:prev_centered")`) show less of an effect of prevalence on their endorsement; this is because dangerous and distinctive properties are endorsed more so than neutral items, specifically at low prevalence levels, thus making their endorsement curves more flat. 
The main effects of dangerous and distinctive properties on endorsement are in the predicted direction, but the credible intervals include zero ($\beta_{dangerous}=$ `r format_regression_effects(brms.endorsement.bern.2.summary, "stim_typedanger")`;  $\beta_{distinctive}=$ `r format_regression_effects(brms.endorsement.bern.2.summary, "stim_typedistinct")`).
Finally there is a main effect of prevalence such that generics about more prevalent features are judged true more often ($\beta_{prevalence}=$ `r format_regression_effects(brms.endorsement.bern.2.summary, "prev_centered")`).  

```{r dangerousEndorsements, fig.asp=0.65, fig.cap="A replication of Cimpian et al. (2010; Expt. 4). Y-axis denotes proportion of participants who endorsed the generic statement for different types of properties (color). X-axis denotes the prevalence of the property within the category given to participants.", fig.width=5, out.width="70%", fig.align="center"}

# d.summary.collapse %>%
#   ungroup() %>%
#   mutate(stim_type = factor(stim_type, levels = c("danger", "distinct", "bare"),
#                             labels = c("dangerous", "distinctive", "control"))) %>%
# ggplot(., aes(x = stim_type,
#                       y = mean, ymin = ci_lower, ymax = ci_upper,
#                       fill = stim_type))+
#   #geom_line(alpha = 0.8, position = position_dodge(4))+
#   geom_bar( position = position_dodge(0.5), width = 0.7, stat = 'identity', color = 'black')+
#   geom_errorbar(position = position_dodge(0.5), width = 0.3, color = 'black')+
#   ylab("Proportion endorsement")+
#   #xlab("Referent prevalence")+
#   scale_y_continuous(limits = c(0, 1), breaks = c(0, 0.5, 1))+
#   #scale_x_reverse()+
#   scale_x_discrete(limits = c("control", "distinctive", "dangerous"))+
#   guides(fill = F,color = F)+
#   theme(axis.text.x = element_blank(), axis.title.x = element_blank())

d.summary %>%
  ungroup() %>%
  mutate(stim_type = factor(stim_type, levels = c("danger", "distinct", "bare"),
                            labels = c("dangerous", "distinctive", "control"))) %>%
ggplot(., aes(x = as.numeric(stim_prevalence), 
                      y = mean, ymin = ci_lower, ymax = ci_upper,
                      fill = stim_type, color = stim_type))+
  geom_line(alpha = 0.3, position = position_dodge(4))+
  geom_errorbar(alpha = 0.3, width = 2,  position = position_dodge(4), color = 'black')+
  geom_point( position = position_dodge(4), shape = 21, color = 'black', size = 2.5)+
  ylab("Proportion endorsement")+
  xlab("Referent prevalence")+
  scale_x_continuous(limits = c(0, 100), breaks = c(10, 30, 50, 70, 90))+
  scale_y_continuous(limits = c(0, 1), breaks = c(0, 0.5, 1))+
  theme(legend.position = c(0.8, 0.25))+
  guides(fill = guide_legend(reverse = F, title = "Property type"),
         color = guide_legend(reverse = F, title = "Property type"))
```

Having replicated the results of @Cimpian2010, we now test whether or not the prevalence priors are altered by the presentation of dangerous information. 

# Experiment 2: Prior elicitation

## Method

### Participants

We recruited 80 participants over MTurk. 
The experiment took about X minutes and participants were compensated \$1.25.
The sample size, exclusion criteria, and primary and secondary analyses were preregistered: https://osf.io/x2nhz/register/5771ca429ad5a1020de2872e. 

### Materials and procedure

The experiment was divided into two blocks. 
In the first block, participants were told that on this island there were one thousand (1000) different species of animals. They were asked to make a prediction about how many different species of animals they thought would have the property.
In the second block, participants were told they scientists came across a particular instance of a novel category with the property (e.g., "Scientists today saw a glippet. It had *property*") and asked to predict how many other instances of the category had the property (e.g., "What percentage of other glippets do you think *have property*?").
We did not randomize blocks because the second asks participants to imagine a category with the property; this would likely bias judgments about the number of categories with the property (the first block).

To serve as an attention check, participants were shown a list of ten properties following the last block of the experiment. 
They were asked to select all and only the properties they had seen. 
The list was composed of five seen properties and five distractor properties which were very similar to the seen properties (e.g., seen properties = \{*yellow legs*, *blue ears*, *pink teeth*, ...\}, distactor properties = \{*red arms*, *purple eyes*, *copper fingers*,...\}).

The materials were the same as in Expt. 1. 
The experiment can be viewed at: http://stanford.edu/~mtessler/generics-topdown/experiments/dangerous-distinct/prior-1.html

## Results

```{r load dangerous priors results}
# brms.priors.zero_inf_beta_max, brms.priors.zero_inf_beta_randInt
load(file = "cached_results/dangerous-priors-randIntSlope.RData")
load(file = "cached_results/dangerous-priors-sansInt-randIntSlope.RData") #brms.priors.zero_inf_beta_max.sansInt
load(file = "cached_results/dangerous-priors-projectibility-randIntSlope.RData")
# brms.priors.project.zeroone_beta_max
brms.priors.zero_inf_beta_max.summary <- summary(brms.priors.zero_inf_beta_max)
brms.priors.zero_inf_beta_max.sansInt.summary <- summary(brms.priors.zero_inf_beta_max.sansInt)
brms.priors.project.zeroone_beta_max.summary <- summary(brms.priors.project.zeroone_beta_max)
```

27 participants were excluded for failing to correctly identify at least 4 out of 5 seen properties and correctly reject at least 4 out of 5 unseen properties.
This left a remaning n=53 participants for these analyses.
Our primary analysis concerns whether or not dangerousness and distinctiveness both influence the number of species predicted to have the property in the same way.
In particular, we predict that both dangerousness and distinctiveness will similarly decrease the number of species expected to have the property (i.e., they both increase the distinctiveness of the property).
To test this, we transformed the raw numerical responses (numbers between 0-1000) into proportions by dividing by 1000 and built a Bayesian regression model using a "zero inflated beta" linking function.
Zero-inflated Beta is appropriate because our dependent measure is proportions between 0 and 1 and because pilot testing revealed the response distributions to be highly non-normal, with substantial probability mass at exactly 0 (i.e., 0 species have the property). 
We use the maximal mixed effects structure, which has by-participant and by-item random effects of intercept and effect of property type.^[
The model syntax is: `response ~ property_type +  (1 + property_type | participant) +  (1 + property_type | property)`, where property_type is either dangerous, distinctiveness, or neither, and property are individual items (e.g., purple feathers).
]

In keeping with our prediction, both distinctive properties ($\beta_{distinctive} =$ `r format_regression_effects(brms.priors.zero_inf_beta_max.summary, "property_typedistinctive")`) and dangerous properties ($\beta_{dangerous} =$ `r format_regression_effects(brms.priors.zero_inf_beta_max.summary, "property_typedangerous")`) were expected to be present in fewer categories in comparison to the control condition (i.e., properties present without any further information). 
Figure\ \@ref(fig:dangerousPriorsResults)A-C shows the empirical distribution of responses for both questions, as well as the means. 
The fact that such properties are present in fewer categories relaxes their truth conditions, leading to increased endorsements at lower prevalence levels, as can be seen by the computational model predictions based on these empirically elicited priors (Figure\ \@ref(fig:dangerousPriorsResults)D).
Thus, the influence of dangerous properties on endorsements of generic sentences [at least the examples used by @Cimpian2010] can be predicted purely to the statistics of the feature.
The statistical knowledge of dangerous properties is not meaningfully different from that of distinctive properties ($\beta_{distinctive} =$ `r format_regression_effects(brms.priors.zero_inf_beta_max.sansInt.summary, "property_typedistinctive")`, $\beta_{dangerous} =$ `r format_regression_effects(brms.priors.zero_inf_beta_max.sansInt.summary, "property_typedangerous")`).

As a secondary analysis, we were interested as to whether dangerous properties were also more *projectible*, as hypothesized by @Leslie2008.
Our question corresponds to the $n=1$ condition in @Nisbett1983, wherein participants are asked to generalize a property from a single observed instance.
We built a similar regression model, which used a "zero and one inflated beta" linking function, which independently models the responses of exactly 0% and exactly 100%, because these are undefined under the beta distribution.
The model had a maximal random-effects structure similar to that for the analysis of the distinctiveness measure.
Though the effect of higher ratings of projectibility in comparison to control properties was numerically positive, the credible intervals were too broad to draw about differences for both dangerous ($\beta_{dangerousness} =$ `r format_regression_effects(brms.priors.project.zeroone_beta_max.summary, "property_typedangerous", 2)`) and distinctive properties ($\beta_{distinctive} =$ `r format_regression_effects(brms.priors.project.zeroone_beta_max.summary, "property_typedistinctive", 2)`).
Thus, we do not have evidence to support the claim that dangerousness leads to stronger projectibility, operationalized in terms of projected prevalence based on observing a single instance with the feature [cf., @Rothbart1978].

```{r dangerousPriorsResults, fig.width=11, fig.cap="Results of Expt. 2b. A: Empirical distributions of responses for the question of how many different categories would be expected to have the property. X-axis is truncated (from extending to a maximum possible response of 1000) to zoom in on the region of most responses. B: Empirical distribution of responses for the projectibility question (given one has feature, how many others have feature?). C: Empirical means for both questions. Error-bars denote 95% bootstrapped confidence intervals. D: Generic endorsement model predictions given these elicited prevalence priors.", fig.asp=0.5}
load(file = "cached_results/dangerous-priors-filtered.RData") # d.priors.filtered 
load(file = "cached_results/dangerous-endorsement-modelSummary.RData") # m.endorsement


gg_color_hue <- function(n) {
  hues = seq(15, 375, length = n + 1)
  hcl(h = hues, l = 65, c = 100)[1:n]
}


fig.dangerous.priors.numcats <- d.priors.filtered %>%
  filter(trial_type == "num_categories") %>%
           mutate(property_type = factor(property_type,
                                        levels = c("bare", "distinctive", "dangerous"),
                                        labels = c("control", "distinctive", "dangerous"))) %>%
  ggplot(., aes( x = response, fill = property_type ))+
  geom_histogram(bins = 20, color = 'black',
                aes(y=(..count..)/tapply(..count..,..PANEL..,sum)[..PANEL..]))+
  scale_x_continuous(limits = c(0, 20), breaks = c(0, 10, 20))+
  scale_y_continuous(limits = c(0, 0.85), breaks = c(0, 0.4, 0.8))+
  scale_fill_manual(values = rev(gg_color_hue(3)))+
  guides(fill = F)+
  facet_wrap(~property_type)+
  ylab("proportion of responses")+
  xlab("predicted number of categories with property")

fig.dangerous.priors.projectibility <-d.priors.filtered %>%
  filter(trial_type == "projectibility") %>%
           mutate(property_type = factor(property_type,
                                        levels = c("bare", "distinctive", "dangerous"),
                                        labels = c("control", "distinctive", "dangerous"))) %>%
  ggplot(., aes( x = response, fill = property_type ))+
  geom_histogram(bins = 20, color = 'black', 
                aes(y=(..count..)/tapply(..count..,..PANEL..,sum)[..PANEL..]))+
  scale_fill_manual(values = rev(gg_color_hue(3)))+
  guides(fill = F)+
  scale_x_continuous(breaks = c(0, 50, 100))+
  scale_y_continuous(limits = c(0, 0.52), breaks = c(0, 0.5))+
  facet_wrap(~property_type)+
  ylab("proportion of responses")+
  xlab("predicted number of instances of category with property")


bar_width = 0.6
fig.dangerous.priors.bars <- ggplot(d.prior.summary %>% ungroup() %>%
         mutate(property_type = factor(property_type,
                                        levels = c("bare", "distinctive", "dangerous"),
                                        labels = c("control", "distinctive", "dangerous")),
                trial_type = factor(trial_type,
                                    levels = c("num_categories","projectibility"),
                                    labels = c("Number of categories",
                                               "Number of instances"))), 
       aes( x = property_type,
            #fill = property_type,
                             y = mean,
                             ymin = ci_lower,
                             ymax = ci_upper, fill = property_type ))+
  geom_bar(stat = "identity", position = position_dodge(bar_width),
           width = bar_width, color = 'black')+
  geom_errorbar(position = position_dodge(bar_width), width = bar_width/2)+
  #scale_x_discrete()+ 
  xlim("control","distinctive", "dangerous")+
  scale_fill_manual(values = rev(gg_color_hue(3)))+
  guides(fill = F)+
  scale_y_continuous(limits = c(0, 100), breaks = c(0, 50, 100))+
  #scale_fill_solarized()
  facet_wrap(~trial_type, scales = 'free', ncol = 2)+
  theme(axis.title.x = element_blank(),
        axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1))+
  ylab("Mean ratings")


fig.dangerous.endorsement <- m.endorsement %>%
  ungroup() %>%
  mutate(stim_type = factor(stim_type, levels = c("dangerous", "distinctive", "bare"),
                            labels = c("dangerous", "distinctive", "control"))) %>%
ggplot(., aes(x = stim_prevalence, 
                      y = MAP, ymin = cred_lower, ymax = cred_upper,
                      fill = stim_type, color = stim_type))+
  geom_line(alpha = 0.3, position = position_dodge(4))+
  geom_errorbar(alpha = 0.3, width = 2,  position = position_dodge(4), color = 'black')+
  geom_point( position = position_dodge(4), shape = 21, color = 'black', size = 2.5)+
  ylab("Endorsement model prediction")+
  xlab("Referent prevalence")+
  scale_x_continuous(limits = c(0, 100), breaks = c(10, 30, 50, 70, 90))+
  scale_y_continuous(limits = c(0, 1), breaks = c(0, 0.5, 1))+
  theme(legend.position = 'bottom', legend.direction = "vertical")+
  coord_fixed(ratio = 70)+
  guides(fill = F, color = F)
  # guides(fill = guide_legend(reverse = F, title = "Property type"),
  #        color = guide_legend(reverse = F, title = "Property type"))

cowplot::plot_grid(
  cowplot::plot_grid( fig.dangerous.priors.numcats, fig.dangerous.priors.projectibility, ncol = 1,
                      labels = c("A","B")),
  cowplot::plot_grid( fig.dangerous.priors.bars, fig.dangerous.endorsement, ncol = 1,
                      labels = c("C","D")),
  # fig.dangerous.priors.bars,
  # fig.dangerous.endorsement,
  labels = c("", ""), 
  nrow = 1,
  rel_widths = c(1.3, 1)
)


```

```{r model data dangerous endorsement merge}
md.dangerous.endorsement <- left_join(d.summary %>% ungroup() %>%
    mutate(stim_type = factor(stim_type, 
                              levels = c("bare", "danger", "distinct"), 
                              labels = c("bare", "dangerous", "distinctive"))),
  m.endorsement)
```

To confirm that these elicited prevalence priors are sufficient to explain the impact of dangerousness on the increased endorsements for generics, we incorporating these prior measurements into our computational model of generic endorsement (Ch. 3).
Following our treatment of the priors for novel generics (Ch. 1 Expt. 1, Ch. 3 Expt. 1), we used a zero-inflated beta distribution shape for the prevalence priors, taking the first prior elicitation question (*number of categories*) as the estimate of the probability mass at 0% prevalence and responses to the second prior elicitation question (*projectibility*) as samples from the Beta distribution.
Figure\ \@ref(fig:dangerousPriorsResults)D shows that the model is able to recapitulate the qualitative effects of dangerousness and distinctiveness on generic endorsement and provide a high degree quantitative fit to the endorsement task data ($r^2(15) =$ `r compute_r2(md.dangerous.endorsement, "MAP", "mean")`; $MSE =$ `r compute_mse(md.dangerous.endorsement, "MAP", "mean")`).

## Bayesian mediation analysis

<!-- # Experiment 3: Transient properties -->

# Discussion

How does conceptual knowledge influence our understanding of generics? 
It has been argued that generics are the mind's way of giving voice to primitive conceptual generalizations, and there are several distinct and irreducible criteria that would grant a true generic claim [@Leslie2007].
Here we show that the proclivity of conceptual knowledge to moderate understanding of generics can be explained in terms of more basic, probabilistic knowledge representations. 
<!-- The knowledge that members of a category are born with a property [@Gelman2007] leads to stronger projectibility to other, future members of the category, in comparison to properties that are acquired through experience. -->
<!-- This stronger projectibility interfaces with the prevalence-based semantics that our model assumes, resulting in increased endorsements. -->
Dangerous properties, we show, are less likely to be present in many categories, which results in a prevalence prior distribution resembling that of a more distinctive property  [@Cimpian2010].

<!-- ## The impact of dangerousness -->

Is there no role of dangerousness *per se* on generic endorsement? 
We deem this an open question.
The empirical evidence that has been offered so far is confounded with simpler statistical mechanisms. 
However, the free form explanations of at least a few participants suggests that intuition of utility of conveying dangeorous information is there. 
One wrote: "I decided that I would rather assume all are dangerous than risk injury/death from the chance I come into contact with the species."
Another: "The guidelines that came naturally to me were these: I only called a generalization true if it was at least 80-90% true. However, I always called the generalizations true whenever I thought that doing so would protect people (ie, such and such animal has dangerous properties), even when they were only 10% true."
These intuitions suggest that other utility information is entering into the endorsement patterns. 

Our model could be supplemented with these additional utility constraints [cf., @Goodman2016; @Yoon2016], but we do not find empirical for that such an alternative model at present.
The intuitions shared by some participants are not shared by all: "I did answer false to all of the questions because everything was under 100%. A definitive statement like "Clobs have dangerous yellow fur" cannot be made for a 90% average because there are still that 10%." or "I tried to stick with saying true when the percentage was 90% because then you can usually make a generalization that all of them have the feature."
The analysis we provide describes what a strong future test for the *dangerousness* hypothesis would look like: controlling for the impact of dangerous information on the prevalence priors, and measuring the corresponding projectibility of the property, does dangerous information still lead to an increase in endorsement?



<!-- What is the impact of dangerousness on generic endorsement?  -->
<!-- We have identified two possible avenues for dangerousness to influence generic endorsement that relates only to the subjective statistics of the feature: distortions in perceived prevalence as well as distinctiveness.  -->
<!-- Still, these mechanisms may not account for the entirety of why someone would endorse generics about low-prevalence dangerous properties.  -->

<!-- ## Predictive prevalence and sufficiently long histories -->

<!-- In our reexamination of @Gelman2007, we found that information about property origins influenced the projectibility of the property into future instances of the category.  -->
<!-- The need to need not just the look current statistics but a *sufficiently long history* of examples has intellectual antecedents in the work of @Cohen1999. -->
<!-- Cohen draws his inspiration from a frequentist interpretation of probability; we draw ours from the Bayesian perspective, but in this case, the two converge on a similar answer.  -->

<!-- In pilot work, we found that a number of similar questions one could ask about projectibility did not all lead to the patterns found in our main experiment.  -->
<!-- In our main experiment, we asked *if a new doble were born today, when they grew up*, how likely would they be to have the property? -->
<!-- In pilot work, we asked three similar questions ($n=15$ for each): "If you were to encounter another doble, how likely would they be to have the property?", "There are 100 other dobles on the planet, how many have the property?", and "There is another set of dobles on the other side of planet, how many of them have the property?". -->
<!-- Seemingly none of these questions resulted in the projectibility patterns we saw for the *new doble born today* question.  -->
<!-- This suggests that our notion of *predictive prevalence* has to do with simultating a very close causal history (i.e., not the dobles on the other side of the planet) in full (i.e., simulate ontogeny starting today).  -->
<!-- This interpetation would be consistent with related findings that participants are willing to endorse generics displayed by a only minority of the category when that minority are all full-grown adults and majority are babies [@Cimpian2010theory]. -->
<!-- These notions may also relate to what it means to be a normal member of the category, what it means to be a *normal doble* [cf., @Nickel2008]. -->

<!-- Include (in the GD) some informal analysis of Cimpian, Gelman, & Brandone (2010), Lang Cogn Process -->
<!-- 

- dangerousness also distorts predictive prevalence?
  - Leslie draws on inspiration about people's predictions about the likelihood of encountering a dangerouesness property (Leslie  2008, Rothbart etal 1978).
  - Also, representativeness heuristic (KT) in bringing rare events to mind.  
  
- does dangerousness infleunce generic understanding beyond rational(ish) probabilities? 
  - the evidence that dangerounsness impacts endorsement is confounded... there is no evidence that dangerousness impacts endorsements outside of the relevant probabilities 
  - Sterken 2015 argues that these statements are false assertions
  - "loose talk" theories
  - what does Leslie have to say about the mechanisms by which dangerous information is connected?
  - Prasada descrbies the "causal connections"
    - is there a difference between "causal" and "principled" connections in our model? (no, the model is agnositc... also no difference for "statistical" connections...)
-->

\newpage

# References


```{r create_r-references}
r_refs(file = "generics.bib")
```

\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}
