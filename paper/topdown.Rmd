---
title             : "Top-down moderators on generic understanding"
shorttitle        : "Top-down moderators on generic understanding"

author: 
  - name          : "Michael Henry Tessler"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "450 Serra Mall, Bldg. 420, Rm. 316, Stanford, CA 94305"
    email         : "mhtessler@stanford.edu"
  - name          : "Noah D. Goodman"
    affiliation   : "1"

affiliation:
  - id            : "1"
    institution   : "Department of Psychology, Stanford University"
    
header-includes:
  - \usepackage{tabularx}
  - \usepackage{multicol}
  - \usepackage{wrapfig}
  - \usepackage{caption}
  - \usepackage{booktabs}
  - \usepackage{amsmath}
  - \usepackage{graphicx}
  - \usepackage{subcaption}
  - \usepackage{longtable}
  - \usepackage{array}
  - \usepackage{multirow}
  
author_note: >
  This manuscript is currently in prep. Comments or suggestions should be directed to MH Tessler.

abstract: |
  Enter abstract here. Each new line herein must be indented, like this line.
  
keywords          : "keywords"
wordcount         : "X"

bibliography      : ["generics.bib"]

figsintext        : yes
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : no
mask              : no

class             : "man"
output            : papaja::apa6_pdf
---
\newcommand{\denote}[1]{\mbox{ $[\![ #1 ]\!]$}}
\newcommand*\diff{\mathop{}\!\mathrm{d}}
\definecolor{Red}{RGB}{255,0,0}
\definecolor{Green}{RGB}{10,200,100}
\definecolor{Blue}{RGB}{10,100,200}

\newcommand{\mht}[1]{{\textcolor{Blue}{[mht: #1]}}}
\newcommand{\ndg}[1]{{\textcolor{Green}{[ndg: #1]}}}
\newcommand{\red}[1]{{\textcolor{Red}{#1}}}


```{r load_packages, include = FALSE}
library(papaja)
library(tidyverse)
library(cowplot)
library(ggthemes)
library(RColorBrewer)
library(ggpirate)
library(brms)
theme_set(theme_few())
```

```{r helper functions}

format_regression_effects <- function(brm_summary, fixed_effect_name, n_digits = 3){
   #print(fixed_effect_name)
   e1 <- brm_summary[["fixed"]][[fixed_effect_name, "Estimate"]]
   e_lower <- brm_summary[["fixed"]][[fixed_effect_name, "l-95% CI"]]
   e_upper <- brm_summary[["fixed"]][[fixed_effect_name, "u-95% CI"]]
   return(paste(
     format(e1, digits = n_digits), " [", 
     format(e_lower, digits = n_digits), ", ", 
     format(e_upper, digits = n_digits), "]", sep = ""))
}

compute_r2 <- function(df,v1, v2, sigfigs = 3){
  return(format(cor(df[[v1]], df[[v2]])^2, digits = sigfigs))
}

compute_mse <- function(df, v1, v2, sigfigs = 3){
  return(format(mean( (df[[v1]]-df[[v2]])^2), digits = sigfigs))
}


```


```{r analysis_preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(echo=FALSE, warning=FALSE, cache=T, message=FALSE, sanitize = T)
```


# Introduction

In the literature on generics, it is long argued that generic meaning cannot be reduced to probabilities or quantification [@Carlson1977; @Carlson1995; @Gelman2007; @Leslie2008; @Prasada2000; @Cimpian2010theory; @Cimpian2010].
These studies (both empirical and theoretical) are limited by their assumption that the literal meaning of generics is directly inspectable, that we can interpret our intuitions or findings about generics without needing to formalize the world knowledge that any language user would be bring to interpret a sentence.
In this chapter, I examine a few of these studies to see, if indeed, they are incompatible with a probability-based semantics. 

I will examine two case studies that demonstrate top-down influence of conceptual knowledge on generic endorsement: the origins and dangerousness of properties.
An old intuition with generics is that at least some express properties that are especially enduring and central to a category [@Lyons1977]. 
Such a *principled connection* between a kind and a property supports both a normative expectation (e.g., *Ks should F*) and explanation of category membership [e.g., *x is a K because it Fs*; @Prasada2006].
@Gelman2007 posit that the way in which an exemplar comes to have a property---the *origins* of a property---can bring about a principled connection between the kind and the property. 
Learning that members of a category (e.g., *dobles*) are born with the property (e.g., born with claws) signals that the kind has a principled connection to the property, which is believed to be what makes the generic ("Dobles have claws") true.
By contrast, observing individuals who acquire the property from the external world (e.g., dobles who find claws on the side of the road and put them on) is less likely to bring about a principled connection, and the generic intuitively is less true. 

Further evidence for how our understanding of generics is tied to our conceptual knowledge comes from the observation that properties that are "striking, appalling, or otherwise gripping" make the generic "...more tolerant [...] to exceptions" [@Leslie2008, p.15].
This argument stems from an attempt to explain why sentences like "Mosquitos carry malaria" and "Tigers eat people" sound like true generic sentences despite the prevalence of the feature in the referent categories being exceptionally low. 
@Cimpian2010 provided empirical support for this claim by testing adult intuitions about generics about novel categories (e.g., "Lorches have dangerous purple feathers"). 
Information about the dangerousness of the property was found to increase participants' propensity to endorse generic sentences when information about the referent prevalence was fixed [e.g., when 30% of lorches have these purple feathers; @Cimpian2010, Expts. 1 & 4]. 

Both @Gelman2007 and @Cimpian2010 measure generic endorsement by having participants judge whether or the sentence is true or false (or, right vs. wrong).
In Chapter 3, we saw how endorsement depends on the referent prevalence (i.e., the prevalence of the property within the referent category) in a nonlinear way relative to the distribution of prevalence of the property within a set of relevant alternative categories. 
Thus, the minimal computational model presented here already has two distinct mechanisms by which theory-based expectations can influence endorsement: the prevalence prior and the referent prevalence.
Prevalence prior distributions exhibit complex structure (often manifesting as multi-modal distributions) and can change dramatically between properties of different types (e.g., biological vs. accidental properties; Ch. 2, Expt. 1; Ch. 3, Expt. 1, Expt. 2) and even within properties that are all of the same conceptual type (e.g., biological properties; Ch. 2, Expt. 2; Ch. 3, Expt. 2). 
Second, we take referent prevalence to be not the objective frequency of the property among actual members of the category in the world, as is assumed in other studies.
Instead, the prevalence that matters for determining the truth or falsity of a generic is a subjective, predictive probability that exists in the mind of the speaker.
(Frequency information is relevant insofar as it informs a speaker's beliefs about the property or event's probability of occurring in the future.)
In our experiments on habitual endorsement (Ch. 3, Expt. 2c), we showed how endorsement patterns can easily be divorced from the objective, present frequency, even taking into account the prior distribution over prevalence.
In those experiments, participants' predictions about the future are what mediated their endorsement of habituals (though again, in a manner relative to the prevalence prior). 

We revisit the effects of property origins and property dangerousness through the computational lens of this thesis.
First, if referent prevalence is what matters for generic endorsement, then the *pricipled connections* thought to result from learning intrinsic property origins in @Gelman2007 would matter only insofar as it increases the subjective probability that a future instance of the category would have the property. 
Second, if information about dangerousness is argued to influence generic endorsement in a way independent of prevalence, then it must be shown that information about dangerousness does not alter the prevalence prior.
We test these assumptions in this chapter.

We first replicate the effects shown in @Gelman2007 and @Cimpian2010 (Expt. 4).
We then design paradigms to measure the predictive prevalence and prevalence priors, respectively. 
We find that the endorsement patterns in these experiments can be accounted for by changes in the underlying prevalence representations that the generic endorsement model posits.

# Experiment 1: Property origins

So far, we have shown that property prevalence is sufficient to formalize the semantics of generic statements as an underspecified scalar denotation.
But what is property prevalence?
If generic language is truly conveying generalizations, it would be useful for it to reflect expectations, not just the current statistics in the world [cf., @Leslie2008].
The current frequency of a property is often a good indicator of future frequency, yet the statistics at any particular moment can be distorted by spurious events.
The causal history of a property should also be taken into account when determining whether the property will be present in future situations.
<!-- Does generic language communicate prevalence in terms of past frequency or future expectations? -->


@Gelman2007 found that adult endorsements of novel generics were sensitive to the origins of the property (i.e., whether or not the creatures were born with or acquired the property through experience).
In their paradigm, participants are told a story about a novel creature (e.g., dobles) and a property of that kind (e.g., have claws).
After being told the origins of the property (i.e., dobles were either born with claws, or found claws and put them on), participants are told about an event that either causes the property to disappear (e.g., they drank a chemical and their claws fell off) or that leaves the property intact (e.g., they drank a yummy drink and felt happy).
The final display of the experiment shows the novel category in their current state: Dobles either with or without claws (depending on the outcome of the event).
@Gelman2007 found that participants' judgments of the generic (e.g., "Dobles have claws") were *insensitive* to the current state (i.e., the outcome of the drinking event: property maintained vs. lost): Participants fully-endorsed the generic when it was in-born, and rejected it when it was acquired.

In Experiment 1a, we use a paradigm similar to that of @Gelman2007 to replicate the effect of property origins on generic endorsement.
We replicate the effect of property origins, but also, discover an effect of the event outcome.
In Experiment 1b, we alter the dependent measure to measure *predictive prevalence*: participants' expectations that future instances of the kind will have the property.
We find that, consistent with our hypothesis, knowledge of the property influences predictive prevalence in such a way that tracks both the replicated main effect of property origins as well as the novel effect of current state.
Furthermore, predictive prevalence tracks with subtle item-level differences, further indicating that it is an intermediate representation between conceptual knowledge and generic endorsement.

## Experiment 1a: Endorsement task

The design of this experiment is based on a study reported in @Gelman2007 with slight modifications.

### Method

#### Participants

We recruited 80 participants over MTurk.  
The sample in @Gelman2007 was 14 undergraduates.
The experiment took about 3 minutes and participants were compensated \$0.35.
<!-- None of the participants completed Expt. 1b as well. -->

#### Procedure and materials

```{r doblesExpt, fig.cap="Overview of Expt. 1 (intrinsic origins, property lost condition shown) A: Participants are first introduced to ten identical exemplars of a novel categories, either with or without a property (here, shown with the target property of being red). B: The origins of the property are described, either they are born with the property or the acquire it through extrinsic means (born with property shown). C: The event is described and event outcome shown (here, property is lost). D: Participants evaluate the generic with exemplars present."}
fig.dobles.1 <- ggdraw() + draw_image("figs/dobles-expt-1a.jpeg", scale = 0.9) +
  theme(plot.margin = unit(c(2,0,0,0), "pt"),
        panel.background = element_rect(colour = "black", size=1))
fig.dobles.2 <- ggdraw() + draw_image("figs/dobles-expt-2a.jpeg", scale = 0.9)+
  theme(plot.margin = unit(c(2,0,0,0), "pt"),
        panel.background = element_rect(colour = "black", size=1)) 
fig.dobles.3 <- ggdraw() + draw_image("figs/dobles-expt-3a.jpeg", scale = 0.95)+
  theme(plot.margin = unit(c(2,0,0,0), "pt"),
        panel.background = element_rect(colour = "black", size=1))
fig.dobles.4 <- ggdraw() + draw_image("figs/dobles-expt-4a.jpeg", scale = 0.95)+
  theme(plot.margin = unit(c(2,0,0,0), "pt"),
        panel.background = element_rect(colour = "black", size=1))

plot_grid(
  fig.dobles.1,
  fig.dobles.2,
  fig.dobles.3,
  fig.dobles.4,
  nrow = 2, labels = c("A","B", "C", "D")
)
```

On each trial, participants read a vignette about a novel creature. For instance,

> These are dobles. [picture of ten dobles with claws] Here is how they grew. They grew up with claws. First they were born, then they got bigger, then they were full size. [picture of a doble with claws, getting bigger and bigger; in some vignettes, the animal was first shown hatching out of an egg with the relevant property already visible] Then one day they drank a bad chemical. They got very sick and this is how they looked. [picture of ten dobles without claws]

The trial proceeded by participants reading the text, and clicking a button to continue to the next part of the story at which time, the images changed according to the example above (Figure\ \@ref(fig:doblesExpt)).

Participants completed four trials: two in which the creatures are born with the property (*intrinsic origins*) and two in which the creatures are shown discovering and acquiring the property (*extrinsic origins* e.g., painting themselves brown).
Property origins were crossed with either the creatures drinking a "bad chemical" and losing the property, or drinking a "yummy drink" and maintaining the property.
The outcome of this event determined the final presentation of images that the participant saw (e.g., either ten dobles with claws or ten without).

With the final display of dobles present (either property lost or maintained), we measured endorsement by asking participants: "Do you agree or disagree that: *generic statement* (e.g., Dobles have claws)".
Participants responded by choosing one of two radio buttons corresponding to agree or disagree.

We used two different types of properties: colors (e.g., "Dobles are green.") and body parts (e.g., "Dobles have claws.").
For each type of property, there were approximately eight different property values (different colors or different body parts for different creatures).
The creatures were either birds, bugs, or fish, with randomly sampled physical dimensions (e.g., sizes of body or tail).
The experiment in full can be viewed at \url{http://stanford.edu/~mtessler/generics/experiments/predictive/predictive-1.html}.

#### Results

```{r load dobles regression results}
load("cached_results/brm_dobles_2way_randInt-subj-proptype.RData") # brm.interaction.fit
load("cached_results/brm_doblesPrev_2way_randInt-subj-proptype.RData") # brm.interaction.prev.fit
brm.interaction.fit.summary <- summary(brm.interaction.fit)
brm.interaction.prev.fit.summary <- summary(brm.interaction.prev.fit)

```

One participant was excluded for self-reporting a native language other than English, leaving n=79 for these analyses.
@Gelman2007 found that participants endorsed the generic when the property was inborn but not when the property was acquired through extrinsic means (i.e., an effect of *property origins*), regardless of how the event turned out (property lost vs. maintained). 
Figure\ \@ref(fig:predictiveDoblesResults)A shows the proportion of participants who endorsed the generic under the four experimental conditions.
To evaluate the effects, we built a Bayesian mixed-effects Bernoulli regression model with fixed-effects of property origins, current state, and their interaction with a random intercept by participant and by property type (color vs. body part). 
As is evident in Figure\ \@ref(fig:predictiveDoblesResults)A, there were effects of both property origins and of event outcome such that participants were more likely to endorse the generic when it was a property that was in-born ($\beta =$ `r format_regression_effects(brm.interaction.fit.summary, "originsintrinsic", 3)`) and when it was a property that was maintained following the drinking event ($\beta =$ `r format_regression_effects(brm.interaction.fit.summary, "event_outcomemaintained", 3)`). 
<!-- The 95% credible intervals indicate that there is at least a 95% probability that the regression coefficients are greater than 0. -->
The interaction term was not different from zero ($\beta =$ `r format_regression_effects(brm.interaction.fit.summary, "originsintrinsic:event_outcomemaintained", 3)`).


<!-- In the original study, the first sentence of each vignette used the possessive "my": "These are my dobles.". -->
<!-- To see whether this second main effect could be attributed to a subgroup of participants making their judgments based only on the final display (which showed creatures with or without the property, depending on the event outcome), we categorized participants according to their pattern of responses.  -->
<!-- Just as many participants endorsed the generic based on the final screen alone (*Only current state*: 19) as there were basing their endorsement on both the outcome and the origins (*Both outcome and origins*: 16). -->
<!-- 36 participants based their responses on only the origins, and 9 participants exhibited some other pattern of responses. -->
<!-- Thus, it seems this effect is not due to a subgroup of participants basing their responses on only the final screen. -->

As an exploratory analysis, we looked to see if these effects were modulated by the kind of property in question (skin color vs. body part). 
Figure\ \@ref(fig:predictiveDoblesResultsProperty)A shows the endorsements split up by the type of property in question.
There is suggestive evidence that the endorsement of a generic for skin color properties (e.g., "Dobles are green") seems to be less sensitive to the outcome of the event (i.e., dobles losing their color as a result of drinking a chemical).

The fact that participants were sensitive to the outcome of the event when endorsing the generic suggests that their beliefs about the stability of these properties influenced their judgments.
For example, participants may believe that the fact that the property can be lost based on drinking a chemical indicates that the property is relatively unstable feature, analagous to heads of hair on adult male humans.
Similarly, the existence of properties in the environment that individuals can come across and acquire is perhaps some evidence that the this external cause of the property is relatively stable. 
The exploratory analysis is consistent with this if participants believe skin color to be a less malleable property than the body part stimuli.
If that is what is leading more participants to endorse the generics about skin color, we would expect to see it reflected in their predictions about future instances of the category having the property, which we measure in Expt. 1b.

The fact that we find a second main effect of event outcome in addition to property origins, while the original only found only a main effect of origins, makes it worth remarking on the differences between our paradigm and the original study by @Gelman2007.
At the end of each vignette, the original study had participants judge two statements in counterbalanced order: "Do my dobles have claws?" and "Do dobles have claws?" (the creatures on the displays were first introduced as "my dobles", whereas in our paradigm, they were simply "dobles").
This explicit contrast may have led participants to provide different answers in the conditions where the property origins conflicted with the event outcome.
Another possibility is that the original study was underpowered to detect the effect of event outcome: The original sample size was 14; ours is 80. 


```{r predictiveDoblesResults, fig.width=8, fig.cap="Expt. 1 results. A: Generic endorsement task (Expt. 1a). B: Predicted prevalence task (Expt. 1b). Error bars represented bootstrapped 95% confidence intervals", fig.asp=0.36, cache=F}
load(file = "cached_results/dobles_endorsement_cis.RData") # d.origins.filtered.summary, d.origins.filtered.property.summary
load(file = "cached_results/dobles_prevalence_cis.RData") # d.origins.prev.filtered.summary, d.origins.prev.filtered.property.summary

bar_width = 0.5

fig.dobles.endorsement <- ggplot(d.origins.filtered.summary, aes(x = origins, y = mean, ymin = ci_lower, ymax = ci_upper, 
                                       fill = event_outcome))+
  geom_bar(stat = 'identity', position = position_dodge(bar_width), color = 'black', width = bar_width)+
  scale_fill_manual(values = c("white", "grey45"))+
  geom_errorbar(position = position_dodge(bar_width), width = bar_width/1.5)+
  scale_y_continuous(limits = c(0, 1), breaks = c(0, 0.5, 1))+
  ylab("Proportion endorsement")+
  xlab("Property origins")+
  guides(fill = F)

fig.dobles.prevalence <- ggplot(d.origins.prev.filtered.summary, 
                                aes(x = origins, y = mean, ymin = ci_lower, ymax = ci_upper, 
                                       fill = event_outcome))+
  geom_bar(stat = 'identity', position = position_dodge(bar_width), color = 'black', width = bar_width)+
  scale_fill_manual(values = c("white", "grey45"))+
  geom_errorbar(position = position_dodge(bar_width), width = bar_width/1.5)+
  scale_y_continuous(limits = c(0, 1), breaks = c(0, 0.5, 1))+
  ylab("Predicted prevalence")+
  xlab("Property origins")+
  guides(fill = guide_legend(title = "Event outcome",reverse = T))

cowplot::plot_grid(
  fig.dobles.endorsement,
  fig.dobles.prevalence,
  nrow = 1,
  labels = c("A", "B"),
  rel_widths = c(1, 1.5)
)
```




```{r predictiveDoblesResultsProperty, fig.width=9, fig.cap="Expt. 1 exploratory analysis breaking down results by the type of property (skin color vs. body part). A: Generic endorsement task (Expt. 1a). B: Predicted prevalence task (Expt. 1b). Error bars represented bootstrapped 95% confidence intervals.", fig.asp=0.35}
fig.dobles.endorsement.property <- ggplot(d.origins.filtered.property.summary, 
                                          aes(x = origins, 
                                              y = mean, ymin = ci_lower, ymax = ci_upper, 
                                       fill = stim_proptype))+
  geom_bar(stat = 'identity', position = position_dodge(bar_width), color = 'black', width = bar_width,
           alpha = 0.8)+
  #scale_fill_manual(values = c("white", "grey45"))+
  scale_fill_solarized()+
  geom_errorbar(position = position_dodge(bar_width), width = bar_width/1.5)+
  scale_y_continuous(limits = c(0, 1), breaks = c(0, 0.5, 1))+
  ylab("Proportion endorsement")+
  xlab("Property origins")+
  facet_wrap(~event_outcome, nrow = 1)+
  guides(fill = F)+
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1))

fig.dobles.prevalence.property <- ggplot(d.origins.prev.filtered.property.summary, 
                                         aes(x = origins, y = mean, ymin = ci_lower, ymax = ci_upper, 
                                       fill = stim_proptype))+
  geom_bar(stat = 'identity', position = position_dodge(bar_width), color = 'black', width = bar_width,
           alpha = 0.8)+
  scale_fill_solarized()+
  geom_errorbar(position = position_dodge(bar_width), width = bar_width/1.5)+
  scale_y_continuous(limits = c(0, 1), breaks = c(0, 0.5, 1))+
  ylab("Predicted prevalence")+
  xlab("Property origins")+
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1))+
  facet_wrap(~event_outcome, nrow = 1)+
  guides(fill = guide_legend(title = "Property type",reverse = T))


cowplot::plot_grid(
  fig.dobles.endorsement.property,
  fig.dobles.prevalence.property,
  nrow = 1,
  labels = c("A", "B"),
  rel_widths = c(1, 1.3)
)
```

<!-- reported a main effect of property origins with no effect of the current state. -->

<!-- \red{ clean up } -->
<!-- %                                         Estimate Std. Error z value        Pr(>|z|)     -->
<!-- %(Intercept)                               -2.9444     0.5130  -5.740 0.0000000094624 *** -->
<!-- %event_outcomemaintained                    2.6931     0.5603   4.807 0.0000015342262 *** -->
<!-- %originsintrinsic                           3.6753     0.5658   6.496 0.0000000000824 *** -->
<!-- %event_outcomemaintained:originsintrinsic   0.2396     0.9400   0.255           0.799     -->

<!-- Our pragmatics model with the elicited predicted prevalence from Expt. 1a made two novel predictions for this experiment: (1) in addition to a main effect of origins, we would find a main effect of event outcome; (2) this effect would be stronger for body part properties than for color properties. -->

<!-- As predicted, we found two main effects (Figure \ref{fig:dobles}b, top). -->
<!-- The main effect of property origins replicated ($\beta=3.6, SE=0.57, z=6.5, p<1\text{e}10$): participants were more likely to endorse the generic when it was about a property that the creature was born with. -->
<!-- In addition, we find a second main effect of event outcome  -->
<!--  ($\beta = 2.69, SE = 0.56, z=4.8, p<1\text{e}5$): participants were more likely to endorse the generic when the property was maintained than when it was lost. -->



<!-- \begin{table}[] -->
<!-- \centering -->
<!-- \begin{tabular}{@{}lllll@{}} -->
<!-- \toprule -->
<!--   & Only origins & Only outcome & Both & Neither \\ \midrule -->
<!-- n & 36           & 19           & 16   & 9       \\ \bottomrule -->
<!-- \end{tabular} -->
<!-- \caption{Numbers of participants ($N=80$) who made their generic endorsement based on different factors.} -->
<!-- \label{tab:dobles} -->
<!-- \end{table} -->




<!-- \begin{figure*} -->
<!-- \begin{tabular}{l l} -->
<!-- (a) $S_2$ model predictions & (b) Human endorsement of generic statements \\ -->
<!-- \\ -->
<!-- \centering -->
<!--     \includegraphics[width=0.5\columnwidth]{figs/dobles-model.pdf} & -->
<!--         \includegraphics[width=0.5\columnwidth]{figs/dobles-results.pdf} \\ -->
<!--       \includegraphics[width=0.5\columnwidth]{figs/dobles-model-byItem.pdf} & -->
<!--       \includegraphics[width=0.5\columnwidth]{figs/dobles-byItem-results.pdf} \\ -->
<!-- \end{tabular} -->
<!--     \caption{ -->
<!--     Prevalence is a predictive probability. -->
<!--     (a) Truth judgment model predictions given the predicted prevalence elicited in Expt.~3a. -->
<!--     (b) Average endorsement of the generic statement in Expt.~3b (replication of Gelman and Bloom, 2007). -->
<!--     Bottom row shows data and predictions broken down by property type. -->
<!--   } -->
<!--   \label{fig:dobles} -->
<!-- \end{figure*} -->


## Experiment 1b: Predictive prevalence elicitation

In this experiment, we test the prediction that the effect of property origins and current state observed in Expt. 1a can be explained by differences in participants' predictions about the property in future instances of the category.

### Method

#### Participants

We recruited 80 participants over MTurk.  
The experiment took about 3 minutes and participants were compensated \$0.35.

#### Procedure and materials

The procedure and materials are exactly the same as in Expt. 1a, with the exception of the dependent measure.
After reading the vignette and being shown the final screen of exemplars, we measured *predictive prevalence* by telling participants: "A new doble was born today. When it becomes full grown, how likely is it that it would *have property* (e.g. have claws)?"
Participants responded using sliders ranging from "very unlikely" to "very likely".

### Results

3 participants were excluded for self-reporting a native language other than English, leaving n=77 for these analyses.
The computational model predicts that generic endorsement is modulated by a speaker's degree of belief in the property being present in future instances of the category. 
In order to evaluate this hypothesis, we built a Bayesian mixed-effects beta regression model with fixed-effects of property origins, current state, and their interaction with a random intercept by participant and by property type (color vs. body part). 
As is evident in Figure\ \@ref(fig:predictiveDoblesResults)B, participants predicted with more certainty that the property would be present in future instances when the property that was in-born ($\beta =$ `r format_regression_effects(brm.interaction.prev.fit.summary, "originsintrinsic", 2)`) and when it was a property that was maintained following the drinking event ($\beta =$ `r format_regression_effects(brm.interaction.prev.fit.summary, "event_outcomemaintained", 2)`). 
<!-- The 95% credible intervals indicate that there is at least a 95% probability that the regression coefficients are greater than 0. -->
The interaction term was not different from zero ($\beta =$ `r format_regression_effects(brm.interaction.prev.fit.summary, "originsintrinsic:event_outcomemaintained", 2)`).

Again, as an exploratory analysis, we break the items down by type of property (skin colors vs. body parts). 
Figure\ \@ref(fig:predictiveDoblesResultsProperty)B shows the parallel pattern of responses, wherein predictions about skin color were less sensitive to the event outcome than predictions about body parts. 

To confirm that our computational model accomodates the generic endorsement data (Expt. 1a), we use the predicted prevalence ratings (Expt. 1b) as the referent prevalence $r$ that the speaker model is trying to communicate: $S(u\mid r)$, and examine the model's predicted endorsements, using the body part and skin color priors elicited in Ch. 2 (Expt. 1a) as the priors in this model.
The generic endorsement model predictions track closely with the empirical generic endorsement data based on origins, outcome, and property type (Figure\ \@ref(fig:dobles-model-scatter); $r^2(8) = 0.96$).^[
 <!-- By extension, the generic endorsement model predictions also track closely with the predicted prevalence ratings. -->
 In this instance, the computational model's predictions are very similar to the predicted prevalence ratings, even though the model predictions depend upon the listener's prevalence prior distribution.
  Since the properties are both biological in nature and, as measured in Ch. 2 (Expt. 1a), are both relatively broad (high variance), we do not observe substantial interactions with the prevalence prior.
]
This is strong evidence that beliefs about property origins influences generic endorsement via influencing beliefs about future prevalence. 


<!-- The computational model's predictions are not necessarily the same as the predicted prevalence values, but instead depend upon the listener's prevalence prior distribution.  -->
<!-- Since the properties are only of two types (color and body part) and both biological in nature, we do not expect substantial interactions with the prevalence prior; futher, since the priors for body part and color priors elicited in Ch. 2. (Expt. 1a) are both relatively broad (high variance), we expect predicted generic endorsement to track predictive prevalence to a large degree. -->

<!-- We, thus, elaborate our theory: -->
<!-- The semantics of generics can be understood as a threshold on property prevalence, and this prevalence is a speaker's subjective belief about what is likely to be the case in the future. -->




```{r dobles-model-scatter, fig.width=6, fig.cap="Computational model predictions for generic endorsements (Expt. 1a) based on predicted prevalence ratings (Expt. 1b). Model predictions also track the predicted prevalence ratings to a high degree."}
ggdraw() + draw_image("figs/dobles-model-data-scatter.pdf", scale = 1)
```



<!-- The average predicted prevalences for the four experimental conditions are shown in Table \ref{tab:predictive}. -->
<!-- We observe a main effect of origins, such that when participants read that the creatures had the property from birth, future creatures  are much more likely to have the property as compared to when the property is acquired. -->
<!-- We see that, in our paradigm, participants are also sensitive to the outcome of the event  (property lost or maintained). -->
<!-- This is surprising given @Gelman2007's reporting of no effect of the event outcome on endorsement. -->
<!-- When participants observe a creature who loses the property by drinking a chemical, they report future members of the category are less likely to have the property. -->
<!-- This inference could be driven by inferences about the property (e.g., learning that the property is an unstable property because you can lose it simply by drinking something) or by inferences about the event (e.g., learning that the "chemical drinking" event is within the space of possibilities for dobles, and thus it could happen in the future). -->


<!-- This is because both color and body part priors are relatively broad, and hence when the property is (predicted to be) more prevalent, the generic has a higher probability of applying (see schematic predictions from Figure \ref{fig:schematic-unif} ``have wings'' for comparison). -->
<!-- We also see that the model predicts a subtle by-item difference, such that the influence of the event outcome (lost or maintained) on generic endorsement is predicted to be stronger for body parts than for color terms (Figure \ref{fig:dobles}a, bottom). -->
<!-- This prediction is mostly due to the predicted prevalence for the conflict conditions (intrinsic-lost and extrinsic-maintained) being subtly different (Table \ref{tab:predictive}, right-most columns). -->



<!-- \begin{table} -->
<!-- \centering -->
<!-- \begin{tabular}{l|l|r|r|r} -->
<!-- \hline -->
<!-- Origins & Event Outcome & Collapsed Mean [95\% CI]  & Color  [95\% CI] & Body Part [95\% CI] \\ -->
<!-- \hline -->
<!-- Extrinsic & Lost  & 0.15 [0.10, 0.21]& 0.13 [0.07, 0.21] & 0.17 [0.09, 0.26] \\ -->
<!-- \hline -->
<!-- Extrinsic & Maintained & 0.32 [0.24, 0.39] & 0.24 [0.16, 0.35] & 0.38 [0.27, 0.50] \\ -->
<!-- \hline -->
<!-- Intrinsic & Lost  & 0.69 [0.62, 0.76] & 0.73 [0.63, 0.83] & 0.66 [0.56, 0.75] \\ -->
<!-- \hline -->
<!-- Intrinsic & Maintained  &0.95 [0.94, 0.97] & 0.96 [0.94, 0.98] & 0.94 [0.91, 0.97] \\ -->
<!-- \hline -->
<!-- \end{tabular} -->
<!-- \caption{Predicted prevalence for the four experimental conditions of Expt.~3a. Right two columns show the summaries broken down by the two types of properties used in the experiment.} -->
<!-- \label{tab:predictive} -->
<!-- \end{table} -->

<!-- We use these predicted probabilities as the prevalence $p$ that the speaker model is trying to communicate: $S(u\mid p)$, and examine the model's predicted truth judgments. -->
<!-- We explore the model's predictions for each origin and event outcome, as well as when the data is split by property type (color vs. body parts).  -->
<!-- For priors $P(p)$, we use the body part and color priors elicited in Ch. 2., Expt. 1a. -->
<!-- We see that the model predictions track closely the predicted prevalence (Figure \ref{fig:dobles}a, top, compare with predicted prevalence in Table \ref{tab:predictive}). -->
<!-- This is because both color and body part priors are relatively broad, and hence when the property is (predicted to be) more prevalent, the generic has a higher probability of applying (see schematic predictions from Figure \ref{fig:schematic-unif} ``have wings'' for comparison). -->
<!-- We also see that the model predicts a subtle by-item difference, such that the influence of the event outcome (lost or maintained) on generic endorsement is predicted to be stronger for body parts than for color terms (Figure \ref{fig:dobles}a, bottom). -->
<!-- This prediction is mostly due to the predicted prevalence for the conflict conditions (intrinsic-lost and extrinsic-maintained) being subtly different (Table \ref{tab:predictive}, right-most columns). -->

<!-- The model thus makes two novel predictions for generic endorsement in the paradigm by @Gelman2007. -->
<!-- We predict that in addition to the main effect of origins, we should see a second main effect of event outcome. -->
<!-- Second, we predict that this effect should be slightly stronger in the case of color properties than in the case of body part properties. -->

<!-- ## Discussion -->




# Experiment 2: The impact of strikingness

@Leslie2007 brings our attention to examples like the following:

1. Sharks attack bathers.
2. Rottweilers maul children. 
3. Mosquitos carry the West Nile Virus.

argueing that these generics are reasonable utterances, even though the actual prevalence of the property is quite low (e.g., less than 1% of mosquitos carry the West Nile Virus), because in each case, the property is somehow *striking, alarming,* or *dangerous*:

> The examples... have something in common: In all of them, the sentence attributes harmful, dangerous, or appalling properties to the kind. More generally, if the property in question is the sort of property of which one would be well served to be forewarned, even if there were only a small chance of encountering it, then generic attributions of the property are intuitively true. (2008, p.15)

@Cimpian2010 found that generics about novel categories (e.g., "Lorches have purple feathers") are endorsed more highly when the property being predicated is *distinctive and dangerous*  (Expt. 1) and when described independently as either distinctive or dangerous (Expt. 4).

The information-theoretic communicative model predicts that these differences in endorsements are mediated by corresponding differences in the prevalence prior distributions.^[
  Another non-mutually exclusive possibility suggested by @Leslie2008 is that a speaker's perception of the prevalence of the feature is altered by virtue of its dangerousness [@Rothbart1978].
  This explanatory mechanism is the same as that explored to explain the effects of property origins in Expt. 1.
]
Indeed, we have already seen in Chapter 3 (generic endorsement) that statements about familiar categories like Sentences 1-3 are endorsed despite low referent prevalence because the prevalence prior distribution reflects that of a distinctive property.
That is, because most animals do not carry the West Nile Virus, the endorsement model is willing to endorse the generic sentence when only a few mosquitos have the property. ^[
Technically, it is both because most animal categories have a prevalence of zero (i.e., 0% of the category have the property) and because even among those categories with a non-zero prevalence, the prevalence of the feature is still very low (e.g., diseases are not expected to be in all or even most members of a species).
]

Here, we examine if the striking or dangerous properties used by @Cimpian2010 do, in fact, alter the prevalence priors to make them resemble that of distinctive properties. 
First, we replicate the findings of @Cimpian2010 (Expt. 4) that dangerousness information increases generic endorsement at low prevalence levels.
Then, we run a prior elicitation paradigm to see if these differences in endorsements can be traced back to differences in the prevalence prior distributions. 
As before, we test the explanatory adequacy of our framework by using our computational model with the prevalence priors to predict generic endorsement.

## Experiment 2a: Truth judgment task

This experiment is a replication of @Cimpian2010 (Expt. 4).

### Method

#### Participants

We recruited 80 participants over MTurk. 
The experimental design is very similar to @Cimpian2010 (Expt. 4), and we chose to have a sample size at least twice as large as the original study (original n=15). 
All participants were native English speakers. 
The experiment took about 4 minutes and participants were compensated \$0.50.

#### Materials and procedure

In order to get participants motivated to reason about novel kinds, they were told they were the resident zoologist of a team of scientists on a recently discovered island with many unknown animals; their task was to provide their expert opinion on questions about these animals.
On each trial, participants were given a statement about a property's prevalence within a novel kind (*referent prevalence*; e.g. "50% of feps have yellow fur."). 
Participants were then asked whether or not they agreed or disagreed with the corresponding generic sentence (e.g. "Feps have yellow fur.").

The materials were taken from @Cimpian2010 (Expt. 4).
The properties were 10 animal body parts (e.g., feathers, scales, shells, tails, ...), and each body part was paired with three different colors, creating a total of 30 properties. 
In addition, sentences conveying the dangerousness and the distinctiveness of the property were created and paired with each item.
Finally, property prevalence was either 10%, 30%, 50%, 70%, and 90%.
For example, a dangerous property statement appeared as:

> 30% of lorches have dangerous purple feathers. These feathers are as sharp as needles and can easily get lodged in you, causing massive bleeding.

Participants were then asked if they agreed or disagreed with the generic: "Lorches have dangerous purple feathers". 
Distinctive information was supplemented with some nondangerous facts.
For example, a distinctive property information appeared as: 

> 30% of lorches have distinctive purple feathers. No other animals on this island have wide, smooth feathers like these.

Participants then evaluated a generic such as "Lorches have distinctive purple feathers". 
The experiment consisted of thirty trials. Participant saw each prevalence level twice paired with each of the three kinds of property (5 prevalence levels x 2 repetitions x 3 property types).

The experiment can be viewed at http://stanford.edu/~mtessler/generics/cbg2010-replication/experiment/experiment-15.html

### Results

```{r load dangerous endorsement results, cache=F}
load("cached_results/dangerous-endorsements-summary.RData") # d.summary
load("cached_results/dangerous-endorsements-randIntTypeeff.RData") # brms.endorsement.bern.1, brms.endorsement.bern.2,
brms.endorsement.bern.2.summary <- summary(brms.endorsement.bern.2)
```

One participant was excluded for self-reporting a native language other than English, leaving n=79 for these analyses.
Figure\ \@ref(fig:dangerousEndorsements) shows the endorsement levels for the three different kinds of properties at the five prevalence levels tested.
We see that both dangerous and distinctive properties have a different pattern of endorsement than the control properties.
This is confirmed by a Bayesian bernoulli regression model with fixed-effects of property type (dangerous vs. distinctive vs. neither), prevalence levels (treated as a continuous variable, centered and scaled), and their interaction.
As well, the model included random intercepts and effects of property type, by participant and by item. 
We find evidence for different slopes such that both dangerous properties ($\beta_{dangerous\times prevalence}=$ `r format_regression_effects(brms.endorsement.bern.2.summary, "stim_typedanger:prev_centered")`) and distinctive properties ($\beta_{distinctive\times prevalence}=$ `r format_regression_effects(brms.endorsement.bern.2.summary, "stim_typedistinct:prev_centered")`) show less of an effect of prevalence on their endorsement; this is because they are endorsed higher specifically at low prevalence levels, thus making their endorsement curves more flat. 
The main effects of dangerous and distinctive properties on endorsement are in the predicted direction, but the credible intervals include zero ($\beta_{dangerous}=$ `r format_regression_effects(brms.endorsement.bern.2.summary, "stim_typedanger")`;  $\beta_{distinctive}=$ `r format_regression_effects(brms.endorsement.bern.2.summary, "stim_typedistinct")`).
Finally there is a main effect of prevalence ($\beta_{prevalence}=$ `r format_regression_effects(brms.endorsement.bern.2.summary, "prev_centered")`).  

```{r dangerousEndorsements, fig.asp=0.65, fig.cap="A replication of Cimpian et al. (2010; Expt. 4). Y-axis denotes proportion of participants who endorsed the generic statement for different types of properties (color). X-axis denotes the prevalence of the property within the category given to participants.", fig.width=5, out.width="70%", fig.align="center"}

# d.summary.collapse %>%
#   ungroup() %>%
#   mutate(stim_type = factor(stim_type, levels = c("danger", "distinct", "bare"),
#                             labels = c("dangerous", "distinctive", "control"))) %>%
# ggplot(., aes(x = stim_type,
#                       y = mean, ymin = ci_lower, ymax = ci_upper,
#                       fill = stim_type))+
#   #geom_line(alpha = 0.8, position = position_dodge(4))+
#   geom_bar( position = position_dodge(0.5), width = 0.7, stat = 'identity', color = 'black')+
#   geom_errorbar(position = position_dodge(0.5), width = 0.3, color = 'black')+
#   ylab("Proportion endorsement")+
#   #xlab("Referent prevalence")+
#   scale_y_continuous(limits = c(0, 1), breaks = c(0, 0.5, 1))+
#   #scale_x_reverse()+
#   scale_x_discrete(limits = c("control", "distinctive", "dangerous"))+
#   guides(fill = F,color = F)+
#   theme(axis.text.x = element_blank(), axis.title.x = element_blank())

d.summary %>%
  ungroup() %>%
  mutate(stim_type = factor(stim_type, levels = c("danger", "distinct", "bare"),
                            labels = c("dangerous", "distinctive", "control"))) %>%
ggplot(., aes(x = as.numeric(stim_prevalence), 
                      y = mean, ymin = ci_lower, ymax = ci_upper,
                      fill = stim_type, color = stim_type))+
  geom_line(alpha = 0.3, position = position_dodge(4))+
  geom_errorbar(alpha = 0.3, width = 2,  position = position_dodge(4), color = 'black')+
  geom_point( position = position_dodge(4), shape = 21, color = 'black', size = 2.5)+
  ylab("Proportion endorsement")+
  xlab("Referent prevalence")+
  scale_x_continuous(limits = c(0, 100), breaks = c(10, 30, 50, 70, 90))+
  scale_y_continuous(limits = c(0, 1), breaks = c(0, 0.5, 1))+
  theme(legend.position = c(0.8, 0.25))+
  guides(fill = guide_legend(reverse = F, title = "Property type"),
         color = guide_legend(reverse = F, title = "Property type"))
```

Having replicated the results of @Cimpian2010, we now test whether or not the prevalence priors are altered by the presentation of dangerous information. 


## Experiment 2b: Prior elicitation

### Method

#### Participants

We recruited 80 participants over MTurk. 
The experiment took about X minutes and participants were compensated \$1.25.
The sample size, exclusion criteria, and primary and secondary analyses were preregistered: https://osf.io/x2nhz/register/5771ca429ad5a1020de2872e. 

#### Materials and procedure

The experiment was divided into two blocks. 
In the first block, participants were told that on this island there were one thousand (1000) different species of animals. They were asked to make a prediction about how many different species of animals they thought would have the property.
In the second block, participants were told they scientists came across a particular instance of a novel category with the property (e.g., "Scientists today saw a glippet. It had *property*") and asked to predict how many other instances of the category had the property (e.g., "What percentage of other glippets do you think *have property*?").
We did not randomize blocks because the second asks participants to imagine a category with the property; this would likely bias judgments about the number of categories with the property (the first block).

To serve as an attention check, participants were shown a list of ten properties following the last block of the experiment. 
They were asked to select all and only the properties they had seen. 
The list was composed of five seen properties and five distractor properties which were very similar to the seen properties (e.g., seen properties = \{*yellow legs*, *blue ears*, *pink teeth*, ...\}, distactor properties = \{*red arms*, *purple eyes*, *copper fingers*,...\}).

The materials were the same as in Expt. 2a. 
The experiment can be viewed at: http://stanford.edu/~mtessler/generics-topdown/experiments/dangerous-distinct/prior-1.html

### Results

```{r load dangerous priors results}
# brms.priors.zero_inf_beta_max, brms.priors.zero_inf_beta_randInt
load(file = "cached_results/dangerous-priors-randIntSlope.RData")
load(file = "cached_results/dangerous-priors-sansInt-randIntSlope.RData") #brms.priors.zero_inf_beta_max.sansInt
load(file = "cached_results/dangerous-priors-projectibility-randIntSlope.RData")
# brms.priors.project.zeroone_beta_max
brms.priors.zero_inf_beta_max.summary <- summary(brms.priors.zero_inf_beta_max)
brms.priors.zero_inf_beta_max.sansInt.summary <- summary(brms.priors.zero_inf_beta_max.sansInt)
brms.priors.project.zeroone_beta_max.summary <- summary(brms.priors.project.zeroone_beta_max)
```

27 participants were excluded for failing to correctly identify at least 4 out of 5 seen properties and correctly reject at least 4 out of 5 unseen properties.
This left a remaning n=53 participants for these analyses.
Our primary analysis concerns whether or not dangerousness and distinctiveness both influence the number of species predicted to have the property in the same way.
In particular, we predict that both dangerousness and distinctiveness will similarly decrease the number of species expected to have the property (i.e., they both increase the distinctiveness of the property).
To test this, we transformed the raw numerical responses (numbers between 0-1000) into proportions by dividing by 1000 and built a Bayesian regression model using a "zero inflated beta" linking function.
Zero-inflated Beta is appropriate because our dependent measure is proportions between 0 and 1 and because pilot testing revealed the response distributions to be highly non-normal, with substantial probability mass at exactly 0 (i.e., 0 species have the property). 
We use the maximal mixed effects structure, which has by-participant and by-item random effects of intercept and effect of property type.^[
The model syntax is: `response ~ property_type +  (1 + property_type | participant) +  (1 + property_type | property)`, where property_type is either dangerous, distinctiveness, or neither, and property are individual items (e.g., purple feathers).
]

In keeping with our prediction, both distinctive properties ($\beta_{distinctive} =$ `r format_regression_effects(brms.priors.zero_inf_beta_max.summary, "property_typedistinctive")`) and dangerous properties ($\beta_{dangerous} =$ `r format_regression_effects(brms.priors.zero_inf_beta_max.summary, "property_typedangerous")`) were expected to be present in fewer categories in comparison to the control condition (i.e., properties present without any further information). 
Figure\ \@ref(fig:dangerousPriorsResults)A-C shows the empirical distribution of responses for both questions, as well as the means. 
The fact that such properties are present in fewer categories relaxes their truth conditions, leading to increased endorsements at lower prevalence levels, as can be seen by the computational model predictions based on these empirically elicited priors (Figure\ \@ref(fig:dangerousPriorsResults)D).
Thus, the influence of dangerous properties on endorsements of generic sentences [at least the examples used by @Cimpian2010] can be attributed purely to the statistics of the feature.
Dangerous properties are distinctive properties (the absolute estimates of the effect for each kind of property are not appreciably different from each other  $\beta_{distinctive} =$ `r format_regression_effects(brms.priors.zero_inf_beta_max.sansInt.summary, "property_typedistinctive")`, $\beta_{dangerous} =$ `r format_regression_effects(brms.priors.zero_inf_beta_max.sansInt.summary, "property_typedangerous")`).

As a secondary analysis, we were interested as to whether dangerous properties were also more *projectible*, as hypothesized by @Leslie2008.
Our question corresponds to the $n=1$ condition in @Nisbett1983, wherein participants are asked to generalize a property from a single observed instance.
We built a similar regression model, which used a "zero and one inflated beta" linking function, which independently models the responses of exactly 0% and exactly 100%, because these are undefined under the beta distribution.
The model had a maximal random-effects structure similar to that for the analysis of the distinctiveness measure.
Though the effect of higher ratings of projectibility in comparison to control properties was numerically positive, the credible intervals were too broad to draw a conclusion for both dangerous ($\beta_{dangerousness} =$ `r format_regression_effects(brms.priors.project.zeroone_beta_max.summary, "property_typedangerous", 2)`) and  distinctive properties ($\beta_{distinctive} =$ `r format_regression_effects(brms.priors.project.zeroone_beta_max.summary, "property_typedistinctive", 2)`).
Thus, we do not have evidence to support the claim that dangerousness leads to stronger projectibility, operationalized in projected prevalence based on observing a single instance with the feature [cf., @Rothbart1978].

```{r dangerousPriorsResults, fig.width=11, fig.cap="Results of Expt. 2b. A: Empirical distributions of responses for the question of how many different categories would be expected to have the property. X-axis is truncated (from extending to a maximum possible response of 1000) to zoom in on the region of most responses. B: Empirical distribution of responses for the projectibility question (given one has feature, how many others have feature?). C: Empirical means for both questions. Error-bars denote 95% bootstrapped confidence intervals. D: Generic endorsement model predictions given these elicited prevalence priors.", fig.asp=0.5}
load(file = "cached_results/dangerous-priors-filtered.RData") # d.priors.filtered 
load(file = "cached_results/dangerous-endorsement-modelSummary.RData") # m.endorsement


gg_color_hue <- function(n) {
  hues = seq(15, 375, length = n + 1)
  hcl(h = hues, l = 65, c = 100)[1:n]
}


fig.dangerous.priors.numcats <- d.priors.filtered %>%
  filter(trial_type == "num_categories") %>%
           mutate(property_type = factor(property_type,
                                        levels = c("bare", "distinctive", "dangerous"),
                                        labels = c("control", "distinctive", "dangerous"))) %>%
  ggplot(., aes( x = response, fill = property_type ))+
  geom_histogram(bins = 20, color = 'black',
                aes(y=(..count..)/tapply(..count..,..PANEL..,sum)[..PANEL..]))+
  scale_x_continuous(limits = c(0, 20), breaks = c(0, 10, 20))+
  scale_y_continuous(limits = c(0, 0.85), breaks = c(0, 0.4, 0.8))+
  scale_fill_manual(values = rev(gg_color_hue(3)))+
  guides(fill = F)+
  facet_wrap(~property_type)+
  ylab("proportion of responses")+
  xlab("predicted number of categories with property")

fig.dangerous.priors.projectibility <-d.priors.filtered %>%
  filter(trial_type == "projectibility") %>%
           mutate(property_type = factor(property_type,
                                        levels = c("bare", "distinctive", "dangerous"),
                                        labels = c("control", "distinctive", "dangerous"))) %>%
  ggplot(., aes( x = response, fill = property_type ))+
  geom_histogram(bins = 20, color = 'black', 
                aes(y=(..count..)/tapply(..count..,..PANEL..,sum)[..PANEL..]))+
  scale_fill_manual(values = rev(gg_color_hue(3)))+
  guides(fill = F)+
  scale_x_continuous(breaks = c(0, 50, 100))+
  scale_y_continuous(limits = c(0, 0.52), breaks = c(0, 0.5))+
  facet_wrap(~property_type)+
  ylab("proportion of responses")+
  xlab("predicted number of instances of category with property")


bar_width = 0.6
fig.dangerous.priors.bars <- ggplot(d.prior.summary %>% ungroup() %>%
         mutate(property_type = factor(property_type,
                                        levels = c("bare", "distinctive", "dangerous"),
                                        labels = c("control", "distinctive", "dangerous")),
                trial_type = factor(trial_type,
                                    levels = c("num_categories","projectibility"),
                                    labels = c("Number of categories",
                                               "Number of instances"))), 
       aes( x = property_type,
            #fill = property_type,
                             y = mean,
                             ymin = ci_lower,
                             ymax = ci_upper, fill = property_type ))+
  geom_bar(stat = "identity", position = position_dodge(bar_width),
           width = bar_width, color = 'black')+
  geom_errorbar(position = position_dodge(bar_width), width = bar_width/2)+
  #scale_x_discrete()+ 
  xlim("control","distinctive", "dangerous")+
  scale_fill_manual(values = rev(gg_color_hue(3)))+
  guides(fill = F)+
  scale_y_continuous(limits = c(0, 100), breaks = c(0, 50, 100))+
  #scale_fill_solarized()
  facet_wrap(~trial_type, scales = 'free', ncol = 2)+
  theme(axis.title.x = element_blank(),
        axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1))+
  ylab("Mean ratings")


fig.dangerous.endorsement <- m.endorsement %>%
  ungroup() %>%
  mutate(stim_type = factor(stim_type, levels = c("dangerous", "distinctive", "bare"),
                            labels = c("dangerous", "distinctive", "control"))) %>%
ggplot(., aes(x = stim_prevalence, 
                      y = MAP, ymin = cred_lower, ymax = cred_upper,
                      fill = stim_type, color = stim_type))+
  geom_line(alpha = 0.3, position = position_dodge(4))+
  geom_errorbar(alpha = 0.3, width = 2,  position = position_dodge(4), color = 'black')+
  geom_point( position = position_dodge(4), shape = 21, color = 'black', size = 2.5)+
  ylab("Endorsement model prediction")+
  xlab("Referent prevalence")+
  scale_x_continuous(limits = c(0, 100), breaks = c(10, 30, 50, 70, 90))+
  scale_y_continuous(limits = c(0, 1), breaks = c(0, 0.5, 1))+
  theme(legend.position = 'bottom', legend.direction = "vertical")+
  coord_fixed(ratio = 70)+
  guides(fill = F, color = F)
  # guides(fill = guide_legend(reverse = F, title = "Property type"),
  #        color = guide_legend(reverse = F, title = "Property type"))

cowplot::plot_grid(
  cowplot::plot_grid( fig.dangerous.priors.numcats, fig.dangerous.priors.projectibility, ncol = 1,
                      labels = c("A","B")),
  cowplot::plot_grid( fig.dangerous.priors.bars, fig.dangerous.endorsement, ncol = 1,
                      labels = c("C","D")),
  # fig.dangerous.priors.bars,
  # fig.dangerous.endorsement,
  labels = c("", ""), 
  nrow = 1,
  rel_widths = c(1.3, 1)
)


```


```{r model data dangerous endorsement merge}
md.dangerous.endorsement <- left_join(d.summary %>% ungroup() %>%
    mutate(stim_type = factor(stim_type, 
                              levels = c("bare", "danger", "distinct"), 
                              labels = c("bare", "dangerous", "distinctive"))),
  m.endorsement)
```

To confirm that these elicited prevalence priors are sufficient to explain the impact of dangerousness on the increased endorsements for generics, we incorporating these prior measurements into our computational model of generic endorsement (Ch. 3).
Following our treatment of the priors for novel generics (Ch. 1 Expt. 1, Ch. 3 Expt. 1), we used a zero-inflated beta distribution shape for the prevalence priors, taking the first prior elicitation question (*number of categories*) as the estimate of the probability mass at 0% prevalence and responses to the second prior elicitation question (*projectibility*) as samples from the Beta distribution.
Figure\ \@ref(fig:dangerousPriorsResults)D shows that the model is able to recapitulate the qualitative effects of dangerousness and distinctiveness on generic endorsement and provide a high degree quantitative fit to the endorsement task data ($r^2(15) =$ `r compute_r2(md.dangerous.endorsement, "MAP", "mean")`; $MSE =$ `r compute_mse(md.dangerous.endorsement, "MAP", "mean")`).

<!-- # Experiment 3: Transient properties -->

# Discussion

How does conceptual knowledge influence our understanding of generics? 
It has been argued that generics are the mind's way of giving voice to primitive conceptual generalizations, and there are several distinct and irreducible criteria that would grant a true generic claim [@Leslie2007].
Here we show that two empirical demonstrations of conceptual knowledge moderating understanding of generics can be explained in terms of more basic, probabilistic knowledge representations. 
The knowledge that members of a category are born with a property [@Gelman2007] leads to stronger projectibility to other, future members of the category, in comparison to properties that are acquired through experience.
This stronger projectibility interfaces with the prevalence-based semantics that our model assumes, resulting in increased endorsements.
Dangerous properties [@Cimpian2010], we show, are also less likely to be present in many categories, which results in a prevalence prior distribution resembling that of a more distinctive property.
These statistical representations are sufficient to explain the influence of these conceptual moderators on generic endorsements. 

## The impact of dangerousness

Is there no role of dangerousness *per se* on generic endorsement? 
We deem this an open question.
The empirical evidence that has been offered so far is confounded with simpler statistical mechanisms. 
However, the free form explanations of at least a few participants suggests that intuition of utility of conveying dangeorous information is there. 
One wrote: "I decided that I would rather assume all are dangerous than risk injury/death from the chance I come into contact with the species."
Another: "The guidelines that came naturally to me were these: I only called a generalization true if it was at least 80-90% true. However, I always called the generalizations true whenever I thought that doing so would protect people (ie, such and such animal has dangerous properties), even when they were only 10% true."
These intuitions suggest that other utility information is entering into the endorsement patterns. 

Our model could be supplemented with these additional utility constraints [cf., @Goodman2016; @Yoon2016], but we do not find empirical for that such an alternative model at present.
The intuitions shared by some participants are not shared by all: "I did answer false to all of the questions because everything was under 100%. A definitive statement like "Clobs have dangerous yellow fur" cannot be made for a 90% average because there are still that 10%." or "I tried to stick with saying true when the percentage was 90% because then you can usually make a generalization that all of them have the feature."
The analysis we provide describes what a strong future test for the *dangerousness* hypothesis would look like: controlling for the impact of dangerous information on the prevalence priors, and measuring the corresponding projectibility of the property, does dangerous information still lead to an increase in endorsement?



<!-- What is the impact of dangerousness on generic endorsement?  -->
<!-- We have identified two possible avenues for dangerousness to influence generic endorsement that relates only to the subjective statistics of the feature: distortions in perceived prevalence as well as distinctiveness.  -->
<!-- Still, these mechanisms may not account for the entirety of why someone would endorse generics about low-prevalence dangerous properties.  -->

## Predictive prevalence and sufficiently long histories

In our reexamination of @Gelman2007, we found that information about property origins influenced the projectibility of the property into future instances of the category. 
The need to need not just the look current statistics but a *sufficiently long history* of examples has intellectual antecedents in the work of @Cohen1999.
Cohen draws his inspiration from a frequentist interpretation of probability; we draw ours from the Bayesian perspective, but in this case, the two converge on a similar answer. 

In pilot work, we found that a number of similar questions one could ask about projectibility did not all lead to the patterns found in our main experiment. 
In our main experiment, we asked *if a new doble were born today, when they grew up*, how likely would they be to have the property?
In pilot work, we asked three similar questions ($n=15$ for each): "If you were to encounter another doble, how likely would they be to have the property?", "There are 100 other dobles on the planet, how many have the property?", and "There is another set of dobles on the other side of planet, how many of them have the property?".
Seemingly none of these questions resulted in the projectibility patterns we saw for the *new doble born today* question. 
This suggests that our notion of *predictive prevalence* has to do with simultating a very close causal history (i.e., not the dobles on the other side of the planet) in full (i.e., simulate ontogeny starting today). 
This interpetation would be consistent with related findings that participants are willing to endorse generics displayed by a only minority of the category when that minority are all full-grown adults and majority are babies [@Cimpian2010theory].
These notions may also relate to what it means to be a normal member of the category, what it means to be a *normal doble* [cf., @Nickel2008].

<!-- Include (in the GD) some informal analysis of Cimpian, Gelman, & Brandone (2010), Lang Cogn Process -->
<!-- 

- dangerousness also distorts predictive prevalence?
  - Leslie draws on inspiration about people's predictions about the likelihood of encountering a dangerouesness property (Leslie  2008, Rothbart etal 1978).
  - Also, representativeness heuristic (KT) in bringing rare events to mind.  
  
- does dangerousness infleunce generic understanding beyond rational(ish) probabilities? 
  - the evidence that dangerounsness impacts endorsement is confounded... there is no evidence that dangerousness impacts endorsements outside of the relevant probabilities 
  - Sterken 2015 argues that these statements are false assertions
  - "loose talk" theories
  - what does Leslie have to say about the mechanisms by which dangerous information is connected?
  - Prasada descrbies the "causal connections"
    - is there a difference between "causal" and "principled" connections in our model? (no, the model is agnositc... also no difference for "statistical" connections...)
-->

\newpage

# References


```{r create_r-references}
r_refs(file = "generics.bib")
```

\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}
